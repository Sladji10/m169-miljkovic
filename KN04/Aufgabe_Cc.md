## Docker Swarm Cluster aufsetzen - SETUP HIGH AVAILABILITY PLATFORM

### Zielsetzung

Ziel dieser Challenge war es, eine produktionsreife, hochverfÃ¼gbare Plattform mit Docker Swarm aufzusetzen. Die Plattform besteht aus einem Cluster mit **5 Nodes** auf **AWS EC2-Instanzen**, verteilt Ã¼ber **zwei Availability Zones (AZs)**.

###  Setup-Details

| Komponente  | Anzahl | Beschreibung |
|-------------|--------|--------------|
| *Manager Nodes* | *3* | *Docker-Swarm-Manager, davon einer `Leader`, zwei `Reachable`* |
| *Worker Nodes*  | *2* | *AusfÃ¼hrung der Container* |
| *Availability Zones* | *2** | *z.B. `us-east-1a`, `us-east-1b`* |
| *Plattform* | *AWS EC2* | *Ubuntu 22.04 LTS, `t2.micro`* |
| *Netzwerke* | *Public + Private* | *Kommunikation Ã¼ber `172.31.0.0/16` CIDR* |
| *Cluster Kommunikation* | *Security Group* | *Inbound-Rules fÃ¼r Ports **22**, **2377**, **7946** (TCP/UDP), **4789** (UDP)* |

### EC2-Instanzen (5x)

- **3 Manager-Nodes**
- **2 Worker-Nodes**
- Verteilt Ã¼ber 2 Availability Zones
- `cloud-init` Script zur automatisierten Docker-Installation:

```bash
#!/bin/bash
apt-get update -y
apt-get install docker.io -y
systemctl enable docker
systemctl start docker
```

### Security Group Inbound-Rules

| Port | Protokoll | Zweck |
|------|-----------|-------|
| 22   | TCP       | SSH-Zugriff |
| 2377 | TCP       | Swarm Manager Kommunikation |
| 7946 | TCP/UDP   | Node-Kommunikation |
| 4789 | UDP       | Overlay Netzwerk |

### Swarm Initialisierung
Auf `manager-1`:
```bash
sudo docker swarm init --advertise-addr 172.31.85.233
```

### Andere Nodes hinzufÃ¼gen
Auf allen anderen Nodes:
```bash
sudo docker swarm join --token SWMTKN-1-2o4v9p5d4cqs6ml7uyhzxgcxjfy0j88spystvlly01aoceoyzj-6h6nxgllffeuih2kqnmujy66n 172.31.85.233:2377
sudo docker swarm join --token SWMTKN-1-2o4v9p5d4cqs6ml7uyhzxgcxjfy0j88spystvlly01aoceoyzj-6h6nxgllffeuih2kqnmujy66n 172.31.85.233:2377
```

### Manager auf Drain setzen:
```bash
sudo docker node update --availability drain q7enflj9rutzb3et4gu1dlj2i
sudo docker node update --availability drain upd4pfzwuko96tbw9ctdq8t4b
sudo docker node update --availability drain n1lzb3op34szbsakzfmytcps6
```

Nur Worker-Nodes sollen Container ausfÃ¼hren dÃ¼rfen.

### Nachweis

### âœ… 5 Nodes sichtbar im Cluster:
```bash
sudo docker node ls
```

screeen

### âœ… Manager und Worker-Nodes verteilt Ã¼ber 2 AZs:

Manager 1 und Worker 1 haben AZ: 

Manager 2, Manager 3 und Worker 2 haben AZ: 

Screenshot

---

### Fragen

### ðŸ”¹ **Was macht der Raft-Algorithmus?**
- Konsensmechanismus zur Manager-Wahl
- Verhindert Inkonsistenzen
- Es braucht ein **Quorum** fÃ¼r Entscheidungen (Mehrheit der Manager)

### ðŸ”¹ **Leader & Reachable**
- `Leader`: aktueller Hauptmanager, koordiniert den Cluster
- `Reachable`: bereit, Leader zu Ã¼bernehmen
- FÃ¤llt der Leader aus â†’ automatischer Leader-Wechsel

### ðŸ”¹ **Drain vs. Active**
- `Drain`: Node erhÃ¤lt keine neuen Container, vorhandene werden migriert
- `Active`: Node darf Container ausfÃ¼hren

## Quellen


